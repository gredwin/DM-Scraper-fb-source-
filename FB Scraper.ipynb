{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import socks\n",
    "import socket\n",
    "socks.set_default_proxy(socks.SOCKS5, \"127.0.0.1\", 9150)\n",
    "socket.socket = socks.socksocket\n",
    "import requests\n",
    "print (requests.get('http://icanhazip.com')).content\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import sys \n",
    "import requests\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('UTF-8')\n",
    "import string\n",
    "import time\n",
    "\n",
    "t0=time.time()\n",
    "#Definitions\n",
    "global commsite, start, end,headers,headersReq\n",
    "commsite = 'http://www.dailymail.co.uk/reader-comments/p/asset/readcomments/'\n",
    "start = '?max=100&offset='\n",
    "end ='&order=desc&rcCache=shout'\n",
    "headers = 'id', 'dateCreated', 'message', 'assetId', 'assetUrl','assetCommentCount', 'userAlias','userLocation','assetHeadline',   'userIdentifier', 'voteRating', 'voteCount', 'hasProfilePicture'\n",
    "headings = 'Comment ID', 'Comment Created Time', 'Text', 'Article ID', 'Article URL','assetCommentCount', 'User Alias','User Location','Article Headline',   'User ID', 'voteRating', 'voteCount', 'hasProfilePicture'\n",
    "headersReq = {\n",
    "    'Host': 'www.dailymail.co.uk',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "}\n",
    "\n",
    "\n",
    "#Precompile RegExs\n",
    "timeRegex = re.compile(r\"(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):.+?Z\")\n",
    "pubRegex = re.compile(r\"<meta property =\\\"article:published_time\\\" content=\\\"(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):.+?\")\n",
    "\n",
    "n=2\n",
    "\n",
    "#UI\n",
    "#URL = raw_input('Enter Full Daily Mail Article URL: ')\n",
    "#article = re.search('\\/article-(\\d+)\\/',URL).group(1)\n",
    "\n",
    "#Setup CSV\n",
    "outfile  = open('DMData.csv', \"wb\")\n",
    "writer = csv.writer(outfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "writer.writerow(headings+('length','artCreatedDate','Elapsed Time (minutes)','AverageWordCount','FacebookURL','Number of Parent Comments'))\n",
    "\n",
    "#Isolate post ids from data.csv\n",
    "fb_ids = []\n",
    "fb_idss= ['1234','1367474646645613', '1368068083252936', '1367411916651886', '1366269283432816']\n",
    "\n",
    "with open('data.csv','rb') as csvfile:\n",
    "    reader = csv.DictReader (csvfile)\n",
    "    for row in reader:\n",
    "        if row['is_post']=='1':\n",
    "            match = re.search('._(.+)',row['id'])\n",
    "            fb_ids.append(match.group(1))\n",
    "\n",
    "def scrape(sec2,art2):\n",
    "#Find Publish date\n",
    "    global firstLine, currentLine, lastLine\n",
    "    #print 'inside scrape loop'\n",
    "    FacebookURL = [fb]\n",
    "    URL = 'http://www.dailymail.co.uk/' + sec2 + '/article-' + art2\n",
    "    req = requests.get(URL,headers=headersReq)\n",
    "    lines = req.text\n",
    "    match = re.search(pubRegex,lines)\n",
    "    if match:\n",
    "        pubtime=match.groups()\n",
    "    pubTime = [match.group(3)+'/'+match.group(2)+'/'+match.group(1)+' '+match.group(4)+':'+match.group(5)]\n",
    "\n",
    "    #First set of comments\n",
    "    reqURL = commsite + art2\n",
    "    req = requests.get (reqURL, headers = headersReq)\n",
    "    html_source = req.text\n",
    "\n",
    "    numOfComments = re.findall ('{\"total\":(\\d+)',html_source)\n",
    "    numOfParents = re.findall (',\\\"parentCommentsCount\":(\\d+),',html_source)\n",
    "    nums = int(numOfComments[0])\n",
    "    nump = int(numOfParents[0])\n",
    "    numpList=[nump]\n",
    "    \n",
    "    lastLine = firstLine + nump-1\n",
    "\n",
    "    #Get rest of comments\n",
    "    messagesLen=[]\n",
    "    #n=exlCounter\n",
    "    for i in range (0, nums,100): \n",
    "        comment = []\n",
    "        req = requests.get (reqURL + start + str(i) + end, headers = headersReq)\n",
    "        data = json.loads(req.text)\n",
    "        for i in range(len(data['payload']['page'])):\n",
    "            for header in headers:\n",
    "                if header == 'dateCreated':\n",
    "                    match = re.search(timeRegex, (data['payload']['page'][i]['dateCreated']))\n",
    "                    if match:\n",
    "                        (data['payload']['page'][i]['dateCreated']) = match.group(3)+'/'+match.group(2)+'/'+match.group(1)+' '+match.group(4)+':'+match.group(5)\n",
    "                comment.append(data['payload']['page'][i].get(header,'N\\\\A'))\n",
    "                if header == 'message':\n",
    "                    words = str(data['payload']['page'][i]['message'])\n",
    "                    words = words.translate(None, string.punctuation)\n",
    "                    words = re.split(r'[^0-9A-Za-z]+',words.strip())\n",
    "                    wordsCount=str(len(words)).split()\n",
    "                    messagesLen.append(len(words))\n",
    "                delta = []\n",
    "            #print 'before delta'\n",
    "            delta = ['=' + '(' + 'B' + str(currentLine) + '-' + 'O' + str(currentLine) + ')' + '*1440'] #MS Excel format\n",
    "            #print 'before avg'\n",
    "            averageWordCount = ['=average($N$'+str(firstLine) + ':' + 'N' + str(lastLine) + ')']\n",
    "            #print 'after avg'\n",
    "            comment=comment+wordsCount+pubTime+delta + averageWordCount+FacebookURL+numpList\n",
    "            writer.writerow(comment)\n",
    "            comment = []\n",
    "            currentLine+=1\n",
    "    firstLine = lastLine +1\n",
    "    #exlCounter+= nump\n",
    "            \n",
    "firstLine = 2 #initiate excel counter for multifile extaction in one csv\n",
    "currentLine = 2\n",
    "lastLine = 2\n",
    "\n",
    "counter = 1\n",
    "successes = []\n",
    "fails= []\n",
    "for id in fb_ids:\n",
    "    try:\n",
    "        fb = 'https://www.facebook.com/DailyMail/posts/'+ id #str(1367031010023310)\n",
    "        #print 'Downloading ' + fb\n",
    "        req = requests.get(fb) #get DM post on fb  \n",
    "        #print 'Searching daim.ai URL'\n",
    "        match = re.search ('https:\\/\\/l\\.facebook.com\\/l\\.php\\?u\\=http%3A%2F%2Fdailym\\.ai\\%2F(.+?)\\&amp;',req.text) #get DM minisite article number from fb\n",
    "        #print 'dailm.ai match found'\n",
    "        article1 = 'http://dailym.ai/' + match.group(1)\n",
    "        req = requests.get(article1)\n",
    "        #print 'Got dail.ai article'\n",
    "        match = re.search ('<meta content=\\\"http:\\/\\/www\\.dailymail\\.co\\.uk\\/(.+)\\/article-(.+?)\\/',req.text) #get full article url\n",
    "        #print 'Seraching for dailymail.co.uk URL'\n",
    "        sec2 = match.group(1)\n",
    "        art2 = match.group(2)\n",
    "\n",
    "        scrape (sec2,art2)\n",
    "        print 'Done with id# ' + id + '.\\t Status: SUCCESS. \\t(',counter,'out of ' + str(len(fb_ids)) + ')'\n",
    "        counter+=1\n",
    "        successes.append(id)\n",
    "#        print (requests.get('http://icanhazip.com')).content\n",
    "\n",
    "    except:\n",
    "        print 'Done with id# ' + id + '.\\t Status: FAIL. \\t(',counter,'out of ' + str(len(fb_ids)) + ')'\n",
    "        counter+=1\n",
    "        fails.append(id)\n",
    "#        print (requests.get('http://icanhazip.com')).content\n",
    "    \n",
    "outfile.close()\n",
    "\n",
    "req.connection.close()\n",
    "\n",
    "t1=time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
